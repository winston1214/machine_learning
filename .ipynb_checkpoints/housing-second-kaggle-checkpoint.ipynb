{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n",
    "submission = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The train data size before dropping Id column: {}'.format(train.shape))\n",
    "print('The test data size before dropping Id column: {}'.format(test.shape))\n",
    "train_id = train.Id\n",
    "test_id = test.Id\n",
    "train.drop('Id',axis=1,inplace=True)\n",
    "test.drop('Id',axis = 1,inplace=True)\n",
    "print('The train data size after dropping Id column : {}'.format(train.shape))\n",
    "print('The test data size after dropping Id column : {}'.format(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.get_dtype_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checking Missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_per(df):\n",
    "    ms=pd.DataFrame(columns=['col','missing'])\n",
    "    idx = 0\n",
    "    for i in range(df.shape[1]):\n",
    "        if df.isnull().sum()[i]>0:\n",
    "            ms.loc[idx,'col'] = df.isnull().sum().index[i]\n",
    "            ms.loc[idx,'missing'] = df.isnull().sum()[i]/df.shape[0] * 100\n",
    "            idx+=1\n",
    "        else:\n",
    "            continue\n",
    "    ms=ms.sort_values(by='missing',ascending=False)\n",
    "    return ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_train = missing_per(train)\n",
    "ms_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,12))\n",
    "sns.barplot(data=ms_train,x='col',y='missing')\n",
    "plt.xlabel('feature')\n",
    "plt.xticks(rotation='60',ha='right')\n",
    "plt.title('train missing rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_test = missing_per(test)\n",
    "ms_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,12))\n",
    "sns.barplot(data=ms_test,x='col',y='missing')\n",
    "plt.xlabel('feature')\n",
    "plt.xticks(rotation='60',ha='right')\n",
    "plt.title('train missing rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "sns.scatterplot(data=train,x='GrLivArea',y='SalePrice')\n",
    "plt.title('SalePrice_GrLivArea correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see outliers. we will remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n",
    "sns.scatterplot(data=train,x='GrLivArea',y='SalePrice')\n",
    "plt.title('SalePrice_GrLivArea correlation after removing outliers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(2,2,figsize=(20,12)) \n",
    "sns.scatterplot(data=train,x='1stFlrSF',y='SalePrice',ax=axes[0,0])\n",
    "axes[0,0].set(title='SalePrice_1stFlrSF correlation')\n",
    "sns.scatterplot(data=train,x='GarageArea',y='SalePrice',ax=axes[0,1])\n",
    "axes[0,1].set(title='SalePrice_GarageArea correlation')\n",
    "sns.scatterplot(data=train,x='TotalBsmtSF',y='SalePrice',ax=axes[1,0])\n",
    "axes[1,0].set(title='SalePrice_ToatalBsmtSF correlation')\n",
    "sns.scatterplot(data=train,x='MasVnrArea',y='SalePrice',ax=axes[1,1])\n",
    "axes[1,1].set(title='SalePrice_MasVnrArea correlation')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the graph, I thought it was mostly linear.\n",
    "\n",
    "So, I will draw regplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 = plt.subplots(figsize=(12,8))\n",
    "sns.scatterplot(data=train,x='GrLivArea',y='SalePrice',ax=ax1)\n",
    "sns.regplot(train['GrLivArea'],y=train['SalePrice'],line_kws={'color':'red'},ax=ax1)\n",
    "ax1.set(title = 'SalePrice_GrLivArea regrssion plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(2,2,figsize=(20,12)) \n",
    "sns.scatterplot(data=train,x='1stFlrSF',y='SalePrice',ax=axes[0,0])\n",
    "sns.regplot(x=train['1stFlrSF'],y=train['SalePrice'],line_kws={'color':'red'},ax=axes[0,0])\n",
    "axes[0,0].set(title='SalePrice_1stFlrSF regression plot')\n",
    "sns.scatterplot(data=train,x='GarageArea',y='SalePrice',ax=axes[0,1])\n",
    "sns.regplot(x=train['GarageArea'],y=train['SalePrice'],line_kws={'color':'red'},ax=axes[0,1])\n",
    "axes[0,1].set(title='SalePrice_GarageArea regression plot')\n",
    "sns.scatterplot(data=train,x='TotalBsmtSF',y='SalePrice',ax=axes[1,0])\n",
    "sns.regplot(x=train['TotalBsmtSF'],y=train['SalePrice'],line_kws={'color':'red'},ax=axes[1,0])\n",
    "axes[1,0].set(title='SalePrice_ToatalBsmtSF regression plot')\n",
    "sns.scatterplot(data=train,x='MasVnrArea',y='SalePrice',ax=axes[1,1])\n",
    "sns.regplot(x=train['MasVnrArea'],y=train['SalePrice'],line_kws={'color':'red'},ax=axes[1,1])\n",
    "axes[1,1].set(title='SalePrice_MasVnrArea regression plot')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,20))\n",
    "mask = np.zeros_like(train.corr(), dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.heatmap(train.corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True, center = 0, mask=mask); \n",
    "plt.title(\"Heatmap of all the Features\", fontsize = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is 83% correlation between **GarageYrBlt** and **YearBuilt**.\n",
    "- 83% correlation between **TotRmsAbvGrd** and **GrLivArea**.\n",
    "- 89% correlation between **GarageCars** and **GarageArea**.\n",
    "- Similarly many other features such as **BsmtUnfSF**, **FullBath** have good correlation with other independent feature but not so much with the dependent feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalized 'SalePrice' feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, skew\n",
    "from scipy import stats\n",
    "plt.style.use('seaborn')\n",
    "sns.distplot(train['SalePrice'] , fit=norm)\n",
    "mu, sigma = norm.fit(train['SalePrice'])\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "stats.probplot(train['SalePrice'],plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['SalePrice'] = np.log1p(train['SalePrice'])\n",
    "sns.distplot(train['SalePrice'],fit=norm)\n",
    "mu,sigma = norm.fit(train['SalePrice'])\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "fig=plt.figure()\n",
    "stats.probplot(train['SalePrice'],plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "train_y= train['SalePrice'].values\n",
    "data = pd.concat([train,test],join='inner')\n",
    "print('Merging data size is {}'.format(data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Missing value processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_all = missing_per(data)\n",
    "ms_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,12))\n",
    "sns.barplot(data=ms_all,x='col',y='missing')\n",
    "plt.xlabel('feature')\n",
    "plt.xticks(rotation=60,ha='right')\n",
    "plt.title('All data missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_val_col = [\"Alley\", \n",
    "                   \"PoolQC\", \n",
    "                   \"MiscFeature\",\n",
    "                   \"Fence\",\n",
    "                   \"FireplaceQu\",\n",
    "                   \"GarageType\",\n",
    "                   \"GarageFinish\",\n",
    "                   \"GarageQual\",\n",
    "                   \"GarageCond\",\n",
    "                   'BsmtQual',\n",
    "                   'BsmtCond',\n",
    "                   'BsmtExposure',\n",
    "                   'BsmtFinType1',\n",
    "                   'BsmtFinType2',\n",
    "                   'MasVnrType']\n",
    "\n",
    "for i in missing_val_col:\n",
    "    data[i] = data[i].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['MSZoning'] = data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "data[\"LotFrontage\"] = data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_val_col2 = ['BsmtFinSF1',\n",
    "                    'BsmtFinSF2',\n",
    "                    'BsmtUnfSF',\n",
    "                    'TotalBsmtSF',\n",
    "                    'BsmtFullBath', \n",
    "                    'BsmtHalfBath', \n",
    "                    'GarageYrBlt',\n",
    "                    'GarageArea',\n",
    "                    'GarageCars',\n",
    "                    'MasVnrArea']\n",
    "\n",
    "for i in missing_val_col2:\n",
    "    data[i] = data[i].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_col3 = ['Utilities',\n",
    "               'Functional',\n",
    "               'Exterior1st',\n",
    "               'Exterior2nd',\n",
    "               'Electrical',\n",
    "               'KitchenQual',\n",
    "               'SaleType']\n",
    "for i in missing_col3:\n",
    "    data[i] = data[i].fillna(data[i].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_per(data) # Can't find missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### box-cox Transformation\n",
    "- How to make non-normalized normalized\n",
    "- The reason for using boxcox1p is because Saleprice feature also used log1p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feats = data.dtypes[data.dtypes != \"object\"].index\n",
    "\n",
    "# Check the skew of all numerical features\n",
    "skewed_feats = data[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
    "skewed = pd.DataFrame({'skew':skewed_feats}).sort_values(by='skew',ascending=False)\n",
    "skewed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import boxcox1p\n",
    "skewed = skewed[abs(skewed)>0.75]\n",
    "lam = 0.15\n",
    "for col in skewed.index:\n",
    "    data[col] = boxcox1p(data[col],lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating Derived Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\n",
    "\n",
    "data['YrBltAndRemod']=data['YearBuilt']+data['YearRemodAdd']\n",
    "\n",
    "data['Total_sqr_footage'] = data['BsmtFinSF1'] +data['BsmtFinSF2'] + data['1stFlrSF'] + data['2ndFlrSF']\n",
    "\n",
    "data['Total_Bathrooms'] =data['FullBath'] + (0.5 * data['HalfBath']) +data['BsmtFullBath'] + (0.5 * data['BsmtHalfBath'])\n",
    "\n",
    "data['Total_porch_sf'] = (data['OpenPorchSF'] + data['3SsnPorch'] +data['EnclosedPorch'] + data['ScreenPorch'] +data['WoodDeckSF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hasPool'] = data['PoolArea'].apply(lambda x: 1 if x>0 else 0)\n",
    "data['hasFireplace'] = data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n",
    "data.drop('Fireplaces',axis=1,inplace=True)\n",
    "data['hasGarage'] = data['GarageArea'].apply(lambda x: 1 if x>0 else 0)\n",
    "data.drop('GarageArea',axis=1,inplace=True)\n",
    "data['hasBsmt'] = data['TotalBsmtSF'].apply(lambda x: 1 if x>0 else 0)\n",
    "data['has2ndfloor'] = data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['PoolQC','Utilities','Street'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- feature processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['MSSubClass','OverallCond','YrSold','MoSold']\n",
    "for i in col:\n",
    "    data[i] = data[i].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "cols = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "        'ExterQual', 'ExterCond','HeatingQC', 'KitchenQual', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "        'LotShape', 'PavedDrive', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
    "        'YrSold', 'MoSold']\n",
    "# LabelEncoder to categorical features\n",
    "le = LabelEncoder()\n",
    "for i in cols:\n",
    "    data[i] = le.fit_transform(data[i].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.get_dummies(data).reset_index(drop=True)\n",
    "print('Final data size is {}'.format(final_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = final_data[:ntrain]\n",
    "test_x = final_data[ntrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overfit_reducer(df):\n",
    "    overfit=[]\n",
    "    for i in df.columns:\n",
    "        counts = df[i].value_counts().iloc[0]\n",
    "        if counts/len(df) * 100 >99.9:\n",
    "            overfit.append(i)\n",
    "        else:\n",
    "            continue\n",
    "    return list(overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfit_features = overfit_reducer(train_x)\n",
    "train_x.drop(overfit_features,axis=1,inplace=True)\n",
    "test_x.drop(overfit_features,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "def rmsle_cv(model):\n",
    "    n_fold = 5\n",
    "    kf = KFold(n_fold, shuffle=True,random_state=42).get_n_splits(train_x)\n",
    "    rmse = np.sqrt(-cross_val_score(model,train_x.values,train_y,scoring='neg_mean_squared_error',cv=kf))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV,RidgeCV,ElasticNetCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n",
    "from lightgbm.sklearn import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso and Ridge may be very sensitive outliers. So we need to use RobustScaler.\n",
    "\n",
    "RobustScaler makes the median to be 0 and the IQR to be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_las=[0.0005,0.0001,0.00005,0.00001]\n",
    "e_ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(),LassoCV(alphas=alpha_las,random_state=42,max_iter=1e7))\n",
    "ridge = make_pipeline(RobustScaler(),RidgeCV(alphas = alpha_las))\n",
    "elastic = make_pipeline(RobustScaler(),ElasticNetCV(max_iter=1e7,alphas=alpha_las,l1_ratio = e_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krr = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "rf = RandomForestRegressor(bootstrap=True,max_depth=70,max_features='auto',min_samples_leaf=4,min_samples_split=10,n_estimators=2200)\n",
    "gra = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "xgb = XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)\n",
    "lgbm = LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the RMSE score of the model for weight adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = [lasso,ridge,elastic,krr,rf,gra,xgb,lgbm]\n",
    "model_name = ['Lasso','Ridge','ElasticNet','KernelRidge','RandomForest','GradientBoost','XGBoost','LGBM']\n",
    "for i,j in zip(model,model_name):\n",
    "    print('{} rmse socre is {}'.format(j,rmsle_cv(i).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ElasticNet model has best score followed by Lasso,Gradient Boost.\n",
    "\n",
    "And RandomForest model has worst score therefore I don't select this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know StackingCVRegressor, you will refer to [http://rasbt.github.io/mlxtend/user_guide/regressor/StackingCVRegressor/](http://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.regressor import StackingCVRegressor\n",
    "stack = StackingCVRegressor(regressors=(ridge,lasso,krr,gra,xgb,lgbm),\n",
    "                           meta_regressor=elastic,use_features_in_secondary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsle_cv(stack).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 모두 fit하고 가중치 준 blended_model_predict 함수 생성 후 predict(trian_x) 하고 점수 확인\n",
    "\n",
    "if 별로면 blended 안쓰고 predict(train_x)한다. 여기서 점수 확인 후 가중치 다시 주고 predict(test_x)\n",
    "\n",
    "아님 쏵 다 제출 blended_model_predict(test_x)도 제출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그 전에 모델의 rmsle 점수 뽑아서 predict(train_x) rmsle 모델 결합 가중치 조정(blend model)\n",
    "- train에서 rmsle 점수 뽑아내고 그 담에 predict(test_x)해서 가중치 비율 조정 후 submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid_fit = ridge.fit(train_x,train_y)\n",
    "lasso_fit = lasso.fit(train_x,train_y)\n",
    "elastic_fit = elastic.fit(train_x,train_y)\n",
    "krr_fit = krr.fit(train_x,train_y)\n",
    "gra_fit = gra.fit(train_x,train_y)\n",
    "xgb_fit = xgb.fit(train_x,train_y)\n",
    "lgbm_fit = lgbm.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_fit = stack.fit(np.array(train_x),np.array(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def rmsle(y,pred_y):\n",
    "    return np.sqrt(mean_squared_error(y,pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y=submission['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1\n",
    "- Blended_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso rmse socre is 0.11285137449325174 - 2 0.2\n",
    "\n",
    "Ridge rmse socre is 0.12369331428588932 - 8 0.05\n",
    "\n",
    "ElasticNet rmse socre is 0.1127704857393996 - 1 0.2\n",
    "\n",
    "KernelRidge rmse socre is 0.11781442034338312 - 7 0.05\n",
    "\n",
    "GradientBoost rmse socre is 0.11461781335632337 - 3 0.15\n",
    "\n",
    "XGBoost rmse socre is 0.11627725614277351 - 5 0.1\n",
    "\n",
    "LGBM rmse socre is 0.1146187308453809 - 4 0.15\n",
    "\n",
    "Stacking 0.11651581352006143 - 6 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blended_model_predict(X):\n",
    "    return ((0.2 * lasso_fit.predict(X))+(0.05*rid_fit.predict(X))+(0.2*elastic_fit.predict(X))+(0.05*krr_fit.predict(X))+\n",
    "           (0.15*gra_fit.predict(X))+(0.1*xgb_fit.predict(X))+ (0.15*lgbm_fit.predict(X))+\n",
    "            (0.1*stack_fit.predict(np.array(X))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Blended model score is {:.5f}'.format(rmsle(train_y,blended_model_predict(train_x))))\n",
    "blend=[]\n",
    "blend.append(rmsle(train_y,blended_model_predict(train_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = submission['Id']\n",
    "test_price_blend = np.expm1(blended_model_predict(test_x))\n",
    "tmp_blend = pd.DataFrame(columns=['Id','SalePrice'])\n",
    "tmp_blend['Id'] = test_id\n",
    "tmp_blend['SalePrice'] = test_price_blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_blend.to_csv('blend_predict.csv',index=False)\n",
    "# 0.11722 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2\n",
    "- Each model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model = [ridge,lasso,elastic,krr,gra,xgb,lgbm]\n",
    "fit_model_name = ['Ridge','Lasso','ElasticNet','KernelRidge','GradientBoost','XGBoost','LGBM']\n",
    "tmp = pd.DataFrame(columns=['model','score'])\n",
    "idx=0\n",
    "for i,j in zip(fit_model,fit_model_name):\n",
    "    pred_y = i.predict(train_x)\n",
    "    print('{} model score is {:.5f}'.format(j,rmsle(train_y,pred_y)))\n",
    "    tmp.loc[idx,'model'] = j\n",
    "    tmp.loc[idx,'score'] = rmsle(train_y,pred_y)\n",
    "    idx+=1\n",
    "pred_stack = stack.predict(np.array(train_x))\n",
    "print('Stacking model score is {:.5f}'.format(rmsle(train_y,pred_stack)))\n",
    "tmp.loc[idx,'model'] = 'Stack'\n",
    "tmp.loc[idx,'score'] = rmsle(train_y,pred_stack)\n",
    "tmp.sort_values(by='score',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_model = 0.7*np.expm1(gra.predict(test_x))+0.15*np.expm1(lgbm.predict(test_x))+0.15*np.expm1(krr.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_mix = pd.DataFrame()\n",
    "tmp_mix['Id'] = test_id\n",
    "tmp_mix['SalePrice'] = mix_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_mix.to_csv('mix_predict.csv',index=False)\n",
    "# 0.11998 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Method 3\n",
    "\n",
    "All mixing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = 0.6*test_price_blend + 0.2*np.expm1(gra.predict(test_x)) +\\\n",
    "0.1*np.expm1(lgbm.predict(test_x))+0.1*np.expm1(krr.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_final = pd.DataFrame()\n",
    "tmp_final['Id'] = test_id\n",
    "tmp_final['SalePrice'] = final_model\n",
    "tmp_final.to_csv('final_predict.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
